# MoE (Mixture of Experts) # 

<div align="center">
  <img src="https://github.com/prodramp/DeepWorks/blob/main/MoE/images/moe-atchitecture.png" width="1000" />
</div> 

<div align="center">
  <img src="https://github.com/prodramp/DeepWorks/blob/main/MoE/images/Moe-processing.png" width="1000" />
</div> 


Advances in deep learning over the last few decades have been driven by a few key elements. With a small number of simple but flexible mechanisms (i.e., inductive biases such as convolutions or sequence attention), increasingly large datasets, and more specialized hardware, neural networks can now achieve impressive results on a wide range of tasks, such as image classification, machine translation, and protein folding prediction.


### Research Papers & GitHub Source Code(s) ###
- https://github.com/davidmrau/mixture-of-experts
- https://github.com/jsuarez5341/Efficient-Dynamic-Batching

### Resources ###
- [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://research.google/pubs/pub45929/)
- [Scaling Vision with Sparse Mixture of Experts](https://ai.googleblog.com/2022/01/scaling-vision-with-sparse-mixture-of.html)
- [LIMoE](https://ai.googleblog.com/2022/06/limoe-learning-multiple-modalities-with.html)
- [Pathways](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture)
- [Vision-MoE](https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html)
- [Task-MoE](https://ai.googleblog.com/2022/01/learning-to-route-by-task-for-efficient.html)

