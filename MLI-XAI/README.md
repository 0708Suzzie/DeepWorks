# Machine Learning Explainability (MLI or XAI = No more blackbox) #

## LIME ##

<div align="center">
  <img src="https://www.oreilly.com/content/wp-content/uploads/sites/2/2019/06/Figure-6-c8db425eefec7cff5a3cf035a40d8841.jpg" width="800" />
  <h6>Figure 6. Explanation for a prediction from Inception. The top three predicted classes are “tree frog,” “pool table,” and “balloon.” Sources: Marco Tulio Ribeiro, Pixabay (frog, billiards, hot air balloon).</h6>
  <hr/>
  <h6>Image Source: https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/</h6>
</div> 

Quick Links:
- https://ema.drwhy.ai/LIME.html
- https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/
- https://dl.acm.org/doi/10.1145/3387166
- https://towardsdatascience.com/deep-learning-model-interpretation-using-shap-a21786e91d16
- https://towardsdatascience.com/explainable-ai-understanding-the-decisions-of-a-convolutional-neural-network-part-1-1a9cf26364fd
- https://medium.com/ing-blog/model-explainability-how-to-choose-the-right-tool-6c5eabd1a46a

GitHub Resources:
- https://github.com/marcotcr/lime
- https://github.com/slundberg/shap
- https://github.com/RikKraanVantage/explainable_AI
