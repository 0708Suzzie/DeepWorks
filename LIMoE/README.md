# LIMoE: Learning Multiple Modalities with One Sparse Mixture-of-Experts Model # 

LIMoE is  large-scale multimodal architecture using a sparse mixture of experts, which simultaneously processes both images and text, but uses sparsely activated experts that naturally specialize.

In this video we are taking a deep dive to learn the more about LIMoE: Learning Multiple Modalities with One Sparse Mixture-of-Experts Model, how it works and internal architecture, text and images data processing..

<table class="table table-striped table-bordered table-vcenter">
    <tr>
        <td align="center"><b>ðŸ”¥&nbsp;YouTube Video:&nbsp;LIMoE: Learning Multiple Modalities with One Sparse Mixture-of-Experts Model</b></td>
    </tr>
    <tr>
        <td>
            <div>
                
[![LIMoE: Learning Multiple Modalities with One Sparse Mixture-of-Experts Model](https://img.youtube.com/vi/i-V33KEwX00/0.jpg)](https://www.youtube.com/watch?v=i-V33KEwX00)

  </tr>
</table>

<div align="center">
  <img src="https://github.com/prodramp/DeepWorks/blob/main/LIMoE/images/limoe.png" width="1000" />
</div> 

### Research Paper and Code 
- [Arxiv: 2206.02770](https://arxiv.org/abs/2206.02770) [PDF](https://arxiv.org/pdf/2206.02770.pdf)
- https://ai.googleblog.com/2022/06/limoe-learning-multiple-modalities-with.html 
- [Improved Multi Perceptor VQGAN + CLIP [Public]](https://colab.research.google.com/drive/1peZ98vBihDD9A1v7JdH5VvHDUuW5tcRK) 


### Resources & Articles ###
- https://medium.com/product-ai/clip-revolution-in-zero-shot-learning-51779b85d10
- https://blog.tensorflow.org/2020/09/introducing-tensorflow-recommenders.html
